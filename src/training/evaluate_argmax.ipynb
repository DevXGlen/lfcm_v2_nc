{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.contingency_tables import mcnemar \n",
    "import csv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcm_result_txt = f\"{os.getenv('ROOT_PATH')}/results/fcm_result.txt\"\n",
    "lfcm_result_txt = f\"{os.getenv('ROOT_PATH')}/results/lfcm_result.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(logits):\n",
    "    logits = np.array(logits)\n",
    "\n",
    "    probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=0)\n",
    "\n",
    "    # Convert probabilities to predictions\n",
    "    predictions = np.argmax(probabilities)\n",
    "\n",
    "    return predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(predicted_values, actual_values):\n",
    "    confusion_matrix = {'tp': 0, 'tn': 0, 'fp': 0, 'fn': 0}\n",
    "    if len(predicted_values) == len(actual_values):\n",
    "        for i in range(len(predicted_values)):\n",
    "            # TP\n",
    "            if predicted_values[i] == 1 and actual_values[i] == 1:\n",
    "                confusion_matrix['tp'] += 1\n",
    "            #  TN\n",
    "            elif predicted_values[i] == 0 and actual_values[i] == 0:\n",
    "                confusion_matrix['tn'] += 1\n",
    "            #  FP\n",
    "            elif predicted_values[i] == 1 and actual_values[i] == 0:\n",
    "                confusion_matrix['fp'] += 1\n",
    "            #  FN\n",
    "            elif predicted_values[i] == 0 and actual_values[i] == 1:\n",
    "                confusion_matrix['fn'] += 1\n",
    "        \n",
    "        return confusion_matrix\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_benchmark_scores(confusion_matrix):\n",
    "    tp = confusion_matrix[\"tp\"]\n",
    "    tn = confusion_matrix[\"tn\"]\n",
    "    fp = confusion_matrix[\"fp\"]\n",
    "    fn = confusion_matrix[\"fn\"]\n",
    "    benchmark_scores = {\"precision\": 0, \"recall\": 0, \"f1-score\": 0, \"accuracy\": 0, 'specificity': 0}\n",
    "\n",
    "    # precision\n",
    "    benchmark_scores[\"precision\"] = tp / (tp + fp)\n",
    "    # recall\n",
    "    benchmark_scores[\"recall\"] = tp / (tp + fn)\n",
    "    # f1-score\n",
    "    benchmark_scores[\"f1-score\"] = (\n",
    "        2 * benchmark_scores[\"precision\"] * benchmark_scores[\"recall\"]\n",
    "    ) / (benchmark_scores[\"precision\"] + benchmark_scores[\"recall\"])\n",
    "    # accuracy\n",
    "    benchmark_scores['accuracy'] = (tp + tn) / (tp + fn + tn + fp)\n",
    "    # specificity\n",
    "    benchmark_scores['specificity'] = tn / (tn + fp)\n",
    "\n",
    "    return benchmark_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mcnemar_contingency_table(a_predicted_values, b_predicted_values, actual_values):\n",
    "    mcnemar_contigency_table = {\"a\": 0, \"b\": 0, \"c\": 0, \"d\": 0}\n",
    "    for i in range(len(actual_values)):\n",
    "        if (\n",
    "            actual_values[i] == a_predicted_values[i]\n",
    "            and actual_values[i] == b_predicted_values[i]\n",
    "        ):\n",
    "            mcnemar_contigency_table[\"a\"] += 1\n",
    "        elif (\n",
    "            actual_values[i] == a_predicted_values[i]\n",
    "            and actual_values[i] != b_predicted_values[i]\n",
    "        ):\n",
    "            mcnemar_contigency_table[\"b\"] += 1\n",
    "        elif (\n",
    "            actual_values[i] != a_predicted_values[i]\n",
    "            and actual_values[i] == b_predicted_values[i]\n",
    "        ):\n",
    "            mcnemar_contigency_table[\"c\"] += 1\n",
    "        elif (\n",
    "            actual_values[i] != a_predicted_values[i]\n",
    "            and actual_values[i] != b_predicted_values[i]\n",
    "        ):\n",
    "            mcnemar_contigency_table[\"d\"] += 1\n",
    "    return mcnemar_contigency_table\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 357, 'tn': 266, 'fp': 682, 'fn': 111}\n",
      "{'precision': 0.343599615014437, 'recall': 0.7628205128205128, 'f1-score': 0.4737889847378899, 'accuracy': 0.4399717514124294, 'specificity': 0.2805907172995781}\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "confusion_matrix_fcm = dict()\n",
    "model_a_actual_values = []\n",
    "model_a_predicted_values = []\n",
    "with open(fcm_result_txt, 'r', encoding='utf-8') as txt_file:\n",
    "    predicted_values = []\n",
    "    actual_values = []\n",
    "    for f in txt_file:\n",
    "        arr = f.split(',')\n",
    "        logits = [float(arr[1]), float(arr[2])]\n",
    "        target = int(arr[0])\n",
    "        total += 1\n",
    "\n",
    "        predicted_values.append(get_prediction(logits))\n",
    "        actual_values.append(target)\n",
    "\n",
    "        # if get_prediction(logits) == target:\n",
    "        #     correct += 1\n",
    "    \n",
    "    confusion_matrix_fcm = get_confusion_matrix(predicted_values, actual_values)\n",
    "    benchmark_score = get_benchmark_scores(confusion_matrix_fcm)\n",
    "    model_a_actual_values = actual_values\n",
    "    model_a_predicted_values = predicted_values\n",
    "\n",
    "    print(confusion_matrix_fcm)\n",
    "    print(benchmark_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 130, 'tn': 695, 'fp': 253, 'fn': 338}\n",
      "{'precision': 0.3394255874673629, 'recall': 0.2777777777777778, 'f1-score': 0.30552291421856637, 'accuracy': 0.5826271186440678, 'specificity': 0.7331223628691983}\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "confusion_matrix_lfcm = dict()\n",
    "model_b_actual_values = []\n",
    "model_b_predicted_values = []\n",
    "with open(lfcm_result_txt, 'r', encoding='utf-8') as txt_file:\n",
    "    predicted_values = []\n",
    "    actual_values = []\n",
    "    for f in txt_file:\n",
    "        arr = f.split(',')\n",
    "        logits = [float(arr[1]), float(arr[2])]\n",
    "        target = int(arr[0])\n",
    "        total += 1\n",
    "\n",
    "        predicted_values.append(get_prediction(logits))\n",
    "        actual_values.append(target)\n",
    "\n",
    "        # if get_prediction(logits) == target:\n",
    "        #     correct += 1\n",
    "    \n",
    "    confusion_matrix_lfcm = get_confusion_matrix(predicted_values, actual_values)\n",
    "    benchmark_score = get_benchmark_scores(confusion_matrix_lfcm)\n",
    "    model_b_actual_values = actual_values\n",
    "    model_b_predicted_values = predicted_values\n",
    "\n",
    "    print(confusion_matrix_lfcm)\n",
    "    print(benchmark_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (model_a_actual_values == model_b_actual_values) and (\n",
    "    len(model_a_predicted_values) == len(model_b_predicted_values)\n",
    "):\n",
    "    mcnemar_contingency_table = get_mcnemar_contingency_table(\n",
    "        model_a_predicted_values, model_b_predicted_values, model_a_actual_values\n",
    "    )\n",
    "    mc_table_arr = [\n",
    "        [mcnemar_contingency_table[\"a\"], mcnemar_contingency_table[\"b\"]],\n",
    "        [mcnemar_contingency_table[\"c\"], mcnemar_contingency_table[\"d\"]],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_ids(csv_file):\n",
    "    tweet_ids = []\n",
    "    with open(csv_file, 'r', encoding='utf-8') as csv_file:\n",
    "        for f in csv_file:\n",
    "            tweet_id = f.split(',')[0]\n",
    "            tweet_ids.append(tweet_id)\n",
    "    return tweet_ids[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_label(label_str):\n",
    "    if (label_str == '0'):\n",
    "        return 'not racist'\n",
    "    elif (label_str == '1'):\n",
    "        return 'racist'\n",
    "    else: return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_5(\n",
    "    tweet_ids=[],\n",
    "    actual_values=[],\n",
    "    a_predicted_values=[],\n",
    "    b_predicted_values=[],\n",
    "    filepath=\"\",\n",
    "):\n",
    "    with open(filepath, mode=\"w\", newline=\"\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['tweet no.', 'tweet filename', 'expected output', 'fcm output', 'lfcm output'])\n",
    "        for i in range(len(tweet_ids)):\n",
    "            a_predicted_value = interpret_label(str(a_predicted_values[i]))\n",
    "            b_predicted_value = interpret_label(str(b_predicted_values[i]))\n",
    "            actual_value = interpret_label(str(actual_values[i]))\n",
    "\n",
    "            row = [\n",
    "                i+1,\n",
    "                f\"{tweet_ids[i]}.json\",\n",
    "                actual_value,\n",
    "                a_predicted_value,\n",
    "                b_predicted_value,\n",
    "            ]\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filepath = f\"{os.getenv('ROOT_PATH')}/test.csv\"\n",
    "table_5_filepath = f\"{os.getenv('RESULTS_PATH')}/table5.csv\"\n",
    "tweet_ids = get_tweet_ids(csv_filepath)\n",
    "actual_values = model_a_actual_values\n",
    "a_pred_values = model_a_predicted_values\n",
    "b_pred_values = model_b_predicted_values\n",
    "\n",
    "create_table_5(tweet_ids, actual_values, a_pred_values, b_pred_values, table_5_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_6(\n",
    "    confusion_matrix_model_a=dict(), confusion_matrix_model_b=dict(), filepath=\"\"\n",
    "):\n",
    "    with open(filepath, mode=\"w\", newline=\"\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(\n",
    "            [\"model name\", \"TP\", \"TN\", \"FP\", \"FN\", \"Pre\", \"Rec\", \"F\", \"Acc\"]\n",
    "        )\n",
    "\n",
    "        a_matrix = confusion_matrix_model_a\n",
    "        b_matrix = confusion_matrix_model_b\n",
    "\n",
    "        model_a_benchmark = get_benchmark_scores(confusion_matrix_model_a)\n",
    "        model_b_benchmark = get_benchmark_scores(confusion_matrix_model_b)\n",
    "\n",
    "        # model b\n",
    "        writer.writerow(\n",
    "            [\n",
    "                b_matrix[\"name\"],\n",
    "                b_matrix[\"tp\"],\n",
    "                b_matrix[\"tn\"],\n",
    "                b_matrix[\"fp\"],\n",
    "                b_matrix[\"fn\"],\n",
    "                model_b_benchmark[\"precision\"],\n",
    "                model_b_benchmark[\"recall\"],\n",
    "                model_b_benchmark[\"f1-score\"],\n",
    "                model_b_benchmark[\"accuracy\"],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # model a\n",
    "        writer.writerow(\n",
    "            [\n",
    "                a_matrix[\"name\"],\n",
    "                a_matrix[\"tp\"],\n",
    "                a_matrix[\"tn\"],\n",
    "                a_matrix[\"fp\"],\n",
    "                a_matrix[\"fn\"],\n",
    "                model_a_benchmark[\"precision\"],\n",
    "                model_a_benchmark[\"recall\"],\n",
    "                model_a_benchmark[\"f1-score\"],\n",
    "                model_a_benchmark[\"accuracy\"],\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_confusion_matrix = confusion_matrix_fcm\n",
    "model_a_confusion_matrix['name'] = 'FCM'\n",
    "\n",
    "model_b_confusion_matrix = confusion_matrix_lfcm\n",
    "model_b_confusion_matrix['name'] = 'LFCM'\n",
    "\n",
    "table_6_filepath = f\"{os.getenv('RESULTS_PATH')}/table6.csv\"\n",
    "\n",
    "create_table_6(model_a_confusion_matrix, model_b_confusion_matrix, table_6_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_7(mc_table, filepath=\"\"):\n",
    "    with open(filepath, mode=\"w\", newline=\"\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"A\", \"B\", \"C\", \"D\", \"McNemar Test Stats\", \"P-Value\"])\n",
    "\n",
    "        _mc_table_arr = [[mc_table[\"a\"], mc_table[\"b\"]], [mc_table[\"c\"], mc_table[\"d\"]]]\n",
    "        _mcnemar = mcnemar(_mc_table_arr, exact=True)\n",
    "\n",
    "        print(_mcnemar)\n",
    "        writer.writerow([mc_table[\"a\"], mc_table[\"b\"], mc_table[\"c\"], mc_table[\"d\"], _mcnemar.statistic, float(str(_mcnemar.pvalue))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvalue      7.644251120860157e-14\n",
      "statistic   265.0\n"
     ]
    }
   ],
   "source": [
    "table_7_filepath = f\"{os.getenv('RESULTS_PATH')}/table7.csv\"\n",
    "\n",
    "create_table_7(mcnemar_contingency_table, table_7_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 120, 'tn': 723, 'fp': 225, 'fn': 348}\n",
      "{'precision': 0.34782608695652173, 'recall': 0.2564102564102564, 'f1-score': 0.2952029520295203, 'accuracy': 0.5953389830508474, 'specificity': 0.7626582278481012}\n"
     ]
    }
   ],
   "source": [
    "c_pred_values = []\n",
    "\n",
    "for i in range(len(a_pred_values)):\n",
    "    if a_pred_values[i] == 1 and b_pred_values[i] == 1:\n",
    "        c_pred_values.append(b_pred_values[i])\n",
    "    elif a_pred_values[i] == 0 and b_pred_values[i] == 0:\n",
    "        c_pred_values.append(b_pred_values[i])\n",
    "    elif b_pred_values[i] == 1 and a_pred_values[i] != 1:\n",
    "        c_pred_values.append(a_pred_values[i])\n",
    "    else:\n",
    "        c_pred_values.append(b_pred_values[i])\n",
    "\n",
    "\n",
    "\n",
    "confusion_matrix_c = get_confusion_matrix(c_pred_values, actual_values)\n",
    "benchmark_score_c = get_benchmark_scores(confusion_matrix_c)\n",
    "\n",
    "print(confusion_matrix_c)\n",
    "print(benchmark_score_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
